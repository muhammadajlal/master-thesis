arch_de: 'ar_transformer_s' # decoder architecture
arch_en: 'blconv_b'
aug: true # whether to perform data augmentation for train set
cache: true # whether to cache the whole dataset into rams
checkpoint: null # path to checkpoint to load
ctc_decoder: best_path
device: cuda
dir_dataset: /home/woody/iwso/iwso214h/imu-hwr/data/wi_sent_hw6_meta
#dir_work: /home/woody/iwso/iwso214h/imu-hwr/results/hwr2/blconv/blconv_b__wi_word_hw6_meta
dir_work: /home/woody/iwso/iwso214h/imu-hwr/results/hwr2/blconv_ARDecoder_no_tokenizer_SameWriter/ar_transformer_s__wi_sent_hw6_meta
#dir_work: /home/woody/iwso/iwso214h/imu-hwr/results/hwr2/blconv/blconv_b__wi_onhw_wd_word_rh

epoch: 300
# We also can set epoch_warmup to 10 for more quick debugging.
epoch_warmup: 30
freeze: false # whether to freeze the encoder
freq_eval: 5 # frequency (epoch) to evaulate model performance on val set
freq_log: 50 # frequency (iteration) to log information
freq_save: 1 # frequency (epoch) to save model checkpoint
idx_fold: 0 # cross validation fold index. 0 for non-cross validation. -1 for train for all cross-validation folds automatically
len_seq: 0
lr: 0.0007
num_channel: 13
num_worker: 16
seed: 42
size_batch: 64
test: false

tokenizer:
  type: sentencepiece
  model: /home/woody/iwso/iwso214h/imu-hwr/work/REWI_work/tokenizer/bpe100.model
  vocab_size: 100
use_bpe: false

# Gating (Only for transformer decoder)
use_gated_attention: true
gating_type: elementwise # options: "elementwise", "headwise"

# Input sequence concatenation for training
concat:
  enabled: true
  # how many samples to concatenate per virtual item
  items_min: 2
  items_max: 4
  # maximum total time steps (budget). Concatenation stops before exceeding this.
  max_T: 8192
  # concatenate only from same writer (recommended)
  same_writer: true
  writer_key: "id_writer"
  # optionally mix concatenated samples with original samples
  # 1.0 = always concatenate; 0.5 = half concat, half original
  p_concat: 0.5
  # keep separators off (as you want)
  use_separator: false
  # recommended: disable base augmentations while concatenating
  # (prevents stacking unrelated augmentations across segments)
  disable_base_aug_in_concat: true



#categories: ["", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z", "Ä", "Ö", "Ü", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z", "ä", "ö", "ü", "ß"] # including seperation label "" for CTC

# For sentence-level dataset
categories: ["", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z", "ä", "ö", "ü", "Ä", "Ö", "Ü", "ß", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", ".", ",", "(",")", "'", "?", "!", "+", "=", "-", "/", ";", ":", "·", " "] 
 