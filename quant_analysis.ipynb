{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 1 — Imports and global configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUT_DIR: ./quant_analysis_outputs_new\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Headless-safe plotting (saves PNGs without needing a display)\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "CSV_PATH = \"./quant_all_val_predictions_new.csv\"\n",
        "CSV_SEP = \";\"\n",
        "OUT_DIR = \"./quant_analysis_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Plot controls\n",
        "X_CLIP = 1.5\n",
        "HIST_BINS = 80\n",
        "\n",
        "# Regime definition: choose ONE\n",
        "REGIME_MODE = \"quantiles\"   # \"quantiles\" OR \"thresholds\"\n",
        "THR = [0.10, 0.30, 0.60]    # only used if REGIME_MODE == \"thresholds\"\n",
        "\n",
        "# Quantile-example extraction\n",
        "K_PER_QUANTILE = 5\n",
        "QUANTILES = [0.50, 0.90, 0.99]\n",
        "TOPK_TAIL = 8\n",
        "MAX_SHOW = 35\n",
        "\n",
        "# Suspicious row verification\n",
        "VERIFY_D_WITH_RECOMPUTE = True\n",
        "\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 2 — Utility helpers (slug + column standardization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _slug(s: str) -> str:\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \"_\", s)\n",
        "    s = re.sub(r\"[^a-z0-9_\\-]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "      - capitalized: \"Task\",\"Fold\",\"Json_path\",\"Sample_index\",\"Prediction\",\"Label\",\"Levenshtein_distance\"\n",
        "      - lowercase:   \"task\",\"fold\",\"json_path\",\"sample_index\",\"prediction\",\"label\",\"levenshtein_distance\"\n",
        "    and normalizes to lowercase canonical names.\n",
        "    \"\"\"\n",
        "    col_map = {c: re.sub(r\"\\s+\", \"\", c).lower() for c in df.columns}\n",
        "\n",
        "    targets = {\n",
        "        \"task\": [\"task\"],\n",
        "        \"fold\": [\"fold\"],\n",
        "        \"json_path\": [\"json_path\", \"jsonpath\"],\n",
        "        \"sample_index\": [\"sample_index\", \"sampleindex\"],\n",
        "        \"prediction\": [\"prediction\", \"pred\"],\n",
        "        \"label\": [\"label\", \"gt\", \"groundtruth\"],\n",
        "        \"levenshtein_distance\": [\"levenshtein_distance\", \"levenshteindistance\", \"lev\", \"distance\"],\n",
        "    }\n",
        "\n",
        "    rename = {}\n",
        "    for original, norm in col_map.items():\n",
        "        for canon, aliases in targets.items():\n",
        "            if norm in aliases:\n",
        "                rename[original] = canon\n",
        "\n",
        "    df = df.rename(columns=rename)\n",
        "\n",
        "    required = [\"task\", \"fold\", \"json_path\", \"sample_index\", \"prediction\", \"label\", \"levenshtein_distance\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            f\"Missing required columns after normalization: {missing}\\n\"\n",
        "            f\"Found columns: {list(df.columns)}\"\n",
        "        )\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 3 — String sanitation helpers + optional Levenshtein recompute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nfc(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFC\", \"\" if pd.isna(s) else str(s))\n",
        "\n",
        "def show_repr(s: str) -> str:\n",
        "    return repr(\"\" if pd.isna(s) else str(s))\n",
        "\n",
        "def trunc(s: str, n: int = MAX_SHOW) -> str:\n",
        "    s = \"\" if pd.isna(s) else str(s)\n",
        "    return s if len(s) <= n else (s[:n] + \"...\")\n",
        "\n",
        "def levenshtein(a: str, b: str) -> int:\n",
        "    \"\"\"Classic DP Levenshtein (edit distance). Use only for small/suspicious subsets.\"\"\"\n",
        "    if a == b:\n",
        "        return 0\n",
        "    la, lb = len(a), len(b)\n",
        "    if la == 0:\n",
        "        return lb\n",
        "    if lb == 0:\n",
        "        return la\n",
        "    if lb > la:\n",
        "        a, b = b, a\n",
        "        la, lb = lb, la\n",
        "    prev = list(range(lb + 1))\n",
        "    for i, ca in enumerate(a, start=1):\n",
        "        cur = [i] + [0] * lb\n",
        "        for j, cb in enumerate(b, start=1):\n",
        "            cost = 0 if ca == cb else 1\n",
        "            cur[j] = min(\n",
        "                prev[j] + 1,\n",
        "                cur[j - 1] + 1,\n",
        "                prev[j - 1] + cost\n",
        "            )\n",
        "        prev = cur\n",
        "    return prev[lb]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 4 — Summary and regimes functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_group(g: pd.DataFrame) -> pd.Series:\n",
        "    n = len(g)\n",
        "\n",
        "    lev_sum = g[\"levenshtein_distance\"].sum()\n",
        "    char_sum = g[\"label_chars\"].sum()\n",
        "    lev_norm_micro = lev_sum / max(1, char_sum)\n",
        "    lev_norm_macro = g[\"lev_norm\"].mean()\n",
        "\n",
        "    vals = g[\"lev_norm\"].to_numpy()\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if len(vals) == 0:\n",
        "        qs = {q: np.nan for q in [0.5, 0.9, 0.95, 0.99]}\n",
        "    else:\n",
        "        qs = {\n",
        "            0.5: np.quantile(vals, 0.5),\n",
        "            0.9: np.quantile(vals, 0.9),\n",
        "            0.95: np.quantile(vals, 0.95),\n",
        "            0.99: np.quantile(vals, 0.99),\n",
        "        }\n",
        "\n",
        "    return pd.Series({\n",
        "        \"N\": n,\n",
        "        \"Exact_match_%\": 100.0 * g[\"exact\"].mean(),\n",
        "        \"Mean_label_chars\": g[\"label_chars\"].mean(),\n",
        "        \"Mean_label_words\": g[\"label_words\"].mean(),\n",
        "        \"LevNorm_micro\": lev_norm_micro,\n",
        "        \"LevNorm_macro\": lev_norm_macro,\n",
        "        \"LevNorm_p50\": qs[0.5],\n",
        "        \"LevNorm_p90\": qs[0.9],\n",
        "        \"LevNorm_p95\": qs[0.95],\n",
        "        \"LevNorm_p99\": qs[0.99],\n",
        "        \"Tail_P(LevNorm>0.5)%\": 100.0 * (g[\"lev_norm\"] > 0.5).mean(),\n",
        "        \"Tail_P(LevNorm>1.0)%\": 100.0 * (g[\"lev_norm\"] > 1.0).mean(),\n",
        "    })\n",
        "\n",
        "def regimes_quantiles(g: pd.DataFrame) -> pd.Series:\n",
        "    vals = g.loc[g[\"lev_norm\"] > 0, \"lev_norm\"].to_numpy()\n",
        "    total = len(g)\n",
        "    exact = (g[\"lev_norm\"] == 0).sum()\n",
        "\n",
        "    if len(vals) == 0:\n",
        "        return pd.Series({\n",
        "            \"Exact\": 100.0, \"Near-miss\": 0.0, \"Moderate\": 0.0, \"Moderate-high\": 0.0, \"Catastrophic\": 0.0,\n",
        "            \"q50_nonzero\": np.nan, \"q90_nonzero\": np.nan, \"q99_nonzero\": np.nan\n",
        "        })\n",
        "\n",
        "    q50 = np.quantile(vals, 0.50)\n",
        "    q90 = np.quantile(vals, 0.90)\n",
        "    q99 = np.quantile(vals, 0.99)\n",
        "\n",
        "    near = ((g[\"lev_norm\"] > 0) & (g[\"lev_norm\"] <= q50)).sum()\n",
        "    mod = ((g[\"lev_norm\"] > q50) & (g[\"lev_norm\"] <= q90)).sum()\n",
        "    mod_hi = ((g[\"lev_norm\"] > q90) & (g[\"lev_norm\"] <= q99)).sum()\n",
        "    cat = (g[\"lev_norm\"] > q99).sum()\n",
        "\n",
        "    return pd.Series({\n",
        "        \"Exact\": 100.0 * exact / total,\n",
        "        \"Near-miss\": 100.0 * near / total,\n",
        "        \"Moderate\": 100.0 * mod / total,\n",
        "        \"Moderate-high\": 100.0 * mod_hi / total,\n",
        "        \"Catastrophic\": 100.0 * cat / total,\n",
        "        \"q50_nonzero\": q50,\n",
        "        \"q90_nonzero\": q90,\n",
        "        \"q99_nonzero\": q99,\n",
        "    })\n",
        "\n",
        "def regimes_thresholds(g: pd.DataFrame, thr=(0.10, 0.30, 0.60)) -> pd.Series:\n",
        "    t1, t2, t3 = thr\n",
        "    total = len(g)\n",
        "\n",
        "    exact = (g[\"lev_norm\"] == 0).sum()\n",
        "    near = ((g[\"lev_norm\"] > 0) & (g[\"lev_norm\"] <= t1)).sum()\n",
        "    mod = ((g[\"lev_norm\"] > t1) & (g[\"lev_norm\"] <= t2)).sum()\n",
        "    mod_hi = ((g[\"lev_norm\"] > t2) & (g[\"lev_norm\"] <= t3)).sum()\n",
        "    cat = (g[\"lev_norm\"] > t3).sum()\n",
        "\n",
        "    return pd.Series({\n",
        "        \"Exact\": 100.0 * exact / total,\n",
        "        \"Near-miss\": 100.0 * near / total,\n",
        "        \"Moderate\": 100.0 * mod / total,\n",
        "        \"Moderate-high\": 100.0 * mod_hi / total,\n",
        "        \"Catastrophic\": 100.0 * cat / total,\n",
        "        \"thr1\": t1, \"thr2\": t2, \"thr3\": t3,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 5 — Plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_exact_rate_by_fold(task: str, g: pd.DataFrame, out_dir: str):\n",
        "    per_fold = g.groupby(\"fold\")[\"exact\"].mean().sort_index() * 100.0\n",
        "    plt.figure()\n",
        "    plt.bar(per_fold.index.astype(int).astype(str), per_fold.values)\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"Exact match (%)\")\n",
        "    plt.title(f\"{task}: Exact match rate by fold\")\n",
        "    out = os.path.join(out_dir, f\"{_slug(task)}_exact_rate_by_fold.png\")\n",
        "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_histograms_and_cdfs(task: str, g: pd.DataFrame, out_dir: str, x_clip: float = 1.5):\n",
        "    vals = g[\"lev_norm\"].to_numpy()\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    task_id = _slug(task)\n",
        "\n",
        "    # Unclipped histogram\n",
        "    plt.figure()\n",
        "    plt.hist(vals, bins=HIST_BINS)\n",
        "    plt.xlabel(\"Normalized Levenshtein (unclipped)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(f\"{task}: histogram of normalized Levenshtein (unclipped)\")\n",
        "    plt.savefig(os.path.join(out_dir, f\"{task_id}_levnorm_hist_unclipped.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    # Clipped histogram\n",
        "    plt.figure()\n",
        "    plt.hist(np.clip(vals, 0, x_clip), bins=HIST_BINS, range=(0, x_clip))\n",
        "    plt.xlabel(f\"Normalized Levenshtein (clipped at {x_clip})\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(f\"{task}: histogram of normalized Levenshtein (clipped)\")\n",
        "    plt.savefig(os.path.join(out_dir, f\"{task_id}_levnorm_hist_clip{x_clip}.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    # Unclipped CDF\n",
        "    s = np.sort(vals)\n",
        "    y = np.arange(1, len(s) + 1) / len(s)\n",
        "    plt.figure()\n",
        "    plt.plot(s, y)\n",
        "    plt.xlabel(\"Normalized Levenshtein (unclipped)\")\n",
        "    plt.ylabel(\"CDF\")\n",
        "    plt.title(f\"{task}: CDF of normalized Levenshtein (unclipped)\")\n",
        "    plt.savefig(os.path.join(out_dir, f\"{task_id}_levnorm_cdf_unclipped.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    # Clipped-window CDF\n",
        "    mask = s <= x_clip\n",
        "    plt.figure()\n",
        "    plt.plot(s[mask], y[mask])\n",
        "    plt.xlabel(f\"Normalized Levenshtein (x ≤ {x_clip})\")\n",
        "    plt.ylabel(\"CDF\")\n",
        "    plt.title(f\"{task}: CDF of normalized Levenshtein (x ≤ {x_clip})\")\n",
        "    plt.savefig(os.path.join(out_dir, f\"{task_id}_levnorm_cdf_xmax{x_clip}.png\"), dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 6 — Quantile-example extraction functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pick_k_closest(nonzero_df: pd.DataFrame, target: float, k: int, used_uids: set) -> pd.DataFrame:\n",
        "    gg = nonzero_df[~nonzero_df[\"uid\"].isin(used_uids)].copy()\n",
        "    if gg.empty:\n",
        "        return gg\n",
        "    gg[\"abs_diff\"] = (gg[\"lev_norm\"] - target).abs()\n",
        "    gg = gg.sort_values([\"abs_diff\", \"lev_norm\"], ascending=[True, True]).head(k).copy()\n",
        "    return gg\n",
        "\n",
        "def quantile_examples_for_task(task_df: pd.DataFrame, k_per_q: int, quantiles: list[float]):\n",
        "    nz = task_df[task_df[\"lev_norm\"] > 0].copy()\n",
        "    if len(nz) == 0:\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    vals = nz[\"lev_norm\"].to_numpy()\n",
        "    qvals = {q: float(np.quantile(vals, q)) for q in quantiles}\n",
        "\n",
        "    used = set()\n",
        "    picked_chunks = []\n",
        "    for q in quantiles:\n",
        "        target = qvals[q]\n",
        "        picked = pick_k_closest(nz, target, k_per_q, used)\n",
        "        if not picked.empty:\n",
        "            picked[\"target_quantile\"] = q\n",
        "            picked[\"target_value\"] = target\n",
        "            picked_chunks.append(picked)\n",
        "            used.update(picked[\"uid\"].tolist())\n",
        "\n",
        "    out = pd.concat(picked_chunks, ignore_index=True) if picked_chunks else pd.DataFrame()\n",
        "    return out, qvals\n",
        "\n",
        "def tail_topk_for_task(task_df: pd.DataFrame, topk: int) -> pd.DataFrame:\n",
        "    nz = task_df[task_df[\"lev_norm\"] > 0].copy()\n",
        "    if nz.empty:\n",
        "        return pd.DataFrame()\n",
        "    return nz.sort_values(\"lev_norm\", ascending=False).head(topk).copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 7 — Load CSV and build core features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task</th>\n",
              "      <th>fold</th>\n",
              "      <th>json_path</th>\n",
              "      <th>sample_index</th>\n",
              "      <th>prediction</th>\n",
              "      <th>label</th>\n",
              "      <th>levenshtein_distance</th>\n",
              "      <th>pred_nfc</th>\n",
              "      <th>label_nfc</th>\n",
              "      <th>label_chars</th>\n",
              "      <th>label_words</th>\n",
              "      <th>lev_norm</th>\n",
              "      <th>exact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>/home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...</td>\n",
              "      <td>0</td>\n",
              "      <td>Du bist dran.</td>\n",
              "      <td>Du bist dran.</td>\n",
              "      <td>0</td>\n",
              "      <td>Du bist dran.</td>\n",
              "      <td>Du bist dran.</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>/home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...</td>\n",
              "      <td>1</td>\n",
              "      <td>Lass/Lasst uns hingehen!; Los, gehen wir hin!</td>\n",
              "      <td>Lass/Lasst uns hingehen!; Los, gehen wir hin!</td>\n",
              "      <td>0</td>\n",
              "      <td>Lass/Lasst uns hingehen!; Los, gehen wir hin!</td>\n",
              "      <td>Lass/Lasst uns hingehen!; Los, gehen wir hin!</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>/home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...</td>\n",
              "      <td>2</td>\n",
              "      <td>Ich hole sie/ihn.</td>\n",
              "      <td>Ich hole sie/ihn.</td>\n",
              "      <td>0</td>\n",
              "      <td>Ich hole sie/ihn.</td>\n",
              "      <td>Ich hole sie/ihn.</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>/home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...</td>\n",
              "      <td>3</td>\n",
              "      <td>Hospital; Krankenhaus</td>\n",
              "      <td>Hospital; Krankenhaus</td>\n",
              "      <td>0</td>\n",
              "      <td>Hospital; Krankenhaus</td>\n",
              "      <td>Hospital; Krankenhaus</td>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>/home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...</td>\n",
              "      <td>4</td>\n",
              "      <td>schon einmal; vorher; zuvor</td>\n",
              "      <td>schon einmal; vorher; zuvor</td>\n",
              "      <td>0</td>\n",
              "      <td>schon einmal; vorher; zuvor</td>\n",
              "      <td>schon einmal; vorher; zuvor</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   task  fold                                          json_path  \\\n",
              "0  sent     0  /home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...   \n",
              "1  sent     0  /home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...   \n",
              "2  sent     0  /home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...   \n",
              "3  sent     0  /home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...   \n",
              "4  sent     0  /home/woody/iwso/iwso214h/imu-hwr/work/REWI_wo...   \n",
              "\n",
              "   sample_index                                     prediction  \\\n",
              "0             0                                  Du bist dran.   \n",
              "1             1  Lass/Lasst uns hingehen!; Los, gehen wir hin!   \n",
              "2             2                              Ich hole sie/ihn.   \n",
              "3             3                          Hospital; Krankenhaus   \n",
              "4             4                    schon einmal; vorher; zuvor   \n",
              "\n",
              "                                           label  levenshtein_distance  \\\n",
              "0                                  Du bist dran.                     0   \n",
              "1  Lass/Lasst uns hingehen!; Los, gehen wir hin!                     0   \n",
              "2                              Ich hole sie/ihn.                     0   \n",
              "3                          Hospital; Krankenhaus                     0   \n",
              "4                    schon einmal; vorher; zuvor                     0   \n",
              "\n",
              "                                        pred_nfc  \\\n",
              "0                                  Du bist dran.   \n",
              "1  Lass/Lasst uns hingehen!; Los, gehen wir hin!   \n",
              "2                              Ich hole sie/ihn.   \n",
              "3                          Hospital; Krankenhaus   \n",
              "4                    schon einmal; vorher; zuvor   \n",
              "\n",
              "                                       label_nfc  label_chars  label_words  \\\n",
              "0                                  Du bist dran.           13            3   \n",
              "1  Lass/Lasst uns hingehen!; Los, gehen wir hin!           45            7   \n",
              "2                              Ich hole sie/ihn.           17            3   \n",
              "3                          Hospital; Krankenhaus           21            2   \n",
              "4                    schon einmal; vorher; zuvor           27            4   \n",
              "\n",
              "   lev_norm  exact  \n",
              "0       0.0   True  \n",
              "1       0.0   True  \n",
              "2       0.0   True  \n",
              "3       0.0   True  \n",
              "4       0.0   True  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(CSV_PATH, sep=CSV_SEP)\n",
        "df = standardize_columns(df)\n",
        "\n",
        "# Types\n",
        "df[\"task\"] = df[\"task\"].astype(str)\n",
        "df[\"fold\"] = pd.to_numeric(df[\"fold\"], errors=\"coerce\")\n",
        "df[\"sample_index\"] = pd.to_numeric(df[\"sample_index\"], errors=\"coerce\")\n",
        "df[\"levenshtein_distance\"] = pd.to_numeric(df[\"levenshtein_distance\"], errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"fold\", \"sample_index\", \"levenshtein_distance\"]).copy()\n",
        "df[\"fold\"] = df[\"fold\"].astype(int)\n",
        "df[\"sample_index\"] = df[\"sample_index\"].astype(int)\n",
        "df[\"levenshtein_distance\"] = df[\"levenshtein_distance\"].astype(int)\n",
        "\n",
        "df[\"prediction\"] = df[\"prediction\"].astype(str)\n",
        "df[\"label\"] = df[\"label\"].astype(str)\n",
        "\n",
        "# NFC versions (for stable length + debug checks)\n",
        "df[\"pred_nfc\"] = df[\"prediction\"].map(nfc)\n",
        "df[\"label_nfc\"] = df[\"label\"].map(nfc)\n",
        "\n",
        "# Lengths and metrics\n",
        "df[\"label_chars\"] = df[\"label_nfc\"].str.len().clip(lower=1)\n",
        "df[\"label_words\"] = df[\"label_nfc\"].str.split().apply(len).clip(lower=1)\n",
        "\n",
        "df[\"lev_norm\"] = df[\"levenshtein_distance\"] / df[\"label_chars\"]\n",
        "df[\"exact\"] = (df[\"levenshtein_distance\"] == 0)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 8 — Overall + foldwise summary CSVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3719459/2811359837.py:1: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  overall = df.groupby(\"task\", as_index=True).apply(summarize_group).sort_index()\n",
            "/tmp/ipykernel_3719459/2811359837.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  foldwise = df.groupby([\"task\", \"fold\"], as_index=True).apply(summarize_group).sort_index()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(            N  Exact_match_%  Mean_label_chars  Mean_label_words  \\\n",
              " task                                                               \n",
              " sent  20639.0      78.235380         18.932119          2.973497   \n",
              " word  24163.0      79.406531          6.525100          1.000000   \n",
              " \n",
              "       LevNorm_micro  LevNorm_macro  LevNorm_p50  LevNorm_p90  LevNorm_p95  \\\n",
              " task                                                                        \n",
              " sent       0.163636       0.169270          0.0     0.823529          1.0   \n",
              " word       0.128366       0.127702          0.0     0.625000          0.8   \n",
              " \n",
              "       LevNorm_p99  Tail_P(LevNorm>0.5)%  Tail_P(LevNorm>1.0)%  \n",
              " task                                                           \n",
              " sent     1.428571             16.929115              4.384902  \n",
              " word     1.142857             12.030791              1.117411  ,\n",
              "                 N  Exact_match_%  Mean_label_chars  Mean_label_words  \\\n",
              " task fold                                                              \n",
              " sent 0     4188.0      79.083095         19.153773          2.984957   \n",
              "      1     3684.0      77.985885         18.885993          2.962541   \n",
              "      2     4416.0      77.943841         18.763587          2.950408   \n",
              "      3     3871.0      81.296823         19.273056          2.971584   \n",
              "      4     4480.0      75.290179         18.634375          2.996205   \n",
              " \n",
              "            LevNorm_micro  LevNorm_macro  LevNorm_p50  LevNorm_p90  \\\n",
              " task fold                                                           \n",
              " sent 0          0.158497       0.161865          0.0     0.809524   \n",
              "      1          0.168305       0.169145          0.0     0.825320   \n",
              "      2          0.167922       0.169606          0.0     0.812500   \n",
              "      3          0.135968       0.143844          0.0     0.777778   \n",
              "      4          0.185154       0.197933          0.0     0.866957   \n",
              " \n",
              "            LevNorm_p95  LevNorm_p99  Tail_P(LevNorm>0.5)%  \\\n",
              " task fold                                                   \n",
              " sent 0        1.000000     1.385171             16.069723   \n",
              "      1        1.000000     1.338485             17.209555   \n",
              "      2        1.000000     1.390301             17.278080   \n",
              "      3        0.941919     1.352941             14.802377   \n",
              "      4        1.071429     1.600000             18.995536   \n",
              " \n",
              "            Tail_P(LevNorm>1.0)%  \n",
              " task fold                        \n",
              " sent 0                 4.202483  \n",
              "      1                 3.990228  \n",
              "      2                 4.257246  \n",
              "      3                 3.642470  \n",
              "      4                 5.647321  )"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overall = df.groupby(\"task\", as_index=True).apply(summarize_group).sort_index()\n",
        "foldwise = df.groupby([\"task\", \"fold\"], as_index=True).apply(summarize_group).sort_index()\n",
        "\n",
        "overall_path = os.path.join(OUT_DIR, \"overall_summary_norm.csv\")\n",
        "foldwise_path = os.path.join(OUT_DIR, \"foldwise_summary_norm.csv\")\n",
        "\n",
        "overall.to_csv(overall_path, index=True)\n",
        "foldwise.to_csv(foldwise_path, index=True)\n",
        "\n",
        "overall, foldwise.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 9 — Regime tables (overall + foldwise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3719459/381814109.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  regimes_overall = df.groupby(\"task\", as_index=True).apply(regimes_quantiles).sort_index()\n",
            "/tmp/ipykernel_3719459/381814109.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  regimes_fold = df.groupby([\"task\", \"fold\"], as_index=True).apply(regimes_quantiles).sort_index()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Exact</th>\n",
              "      <th>Near-miss</th>\n",
              "      <th>Moderate</th>\n",
              "      <th>Moderate-high</th>\n",
              "      <th>Catastrophic</th>\n",
              "      <th>q50_nonzero</th>\n",
              "      <th>q90_nonzero</th>\n",
              "      <th>q99_nonzero</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>task</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sent</th>\n",
              "      <td>78.235380</td>\n",
              "      <td>11.134260</td>\n",
              "      <td>8.479093</td>\n",
              "      <td>1.938078</td>\n",
              "      <td>0.213189</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.230769</td>\n",
              "      <td>1.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <td>79.406531</td>\n",
              "      <td>10.437446</td>\n",
              "      <td>9.038613</td>\n",
              "      <td>0.984977</td>\n",
              "      <td>0.132434</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Exact  Near-miss  Moderate  Moderate-high  Catastrophic  \\\n",
              "task                                                                \n",
              "sent  78.235380  11.134260  8.479093       1.938078      0.213189   \n",
              "word  79.406531  10.437446  9.038613       0.984977      0.132434   \n",
              "\n",
              "      q50_nonzero  q90_nonzero  q99_nonzero  \n",
              "task                                         \n",
              "sent          0.8     1.230769     1.777778  \n",
              "word          0.6     1.000000     1.500000  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if REGIME_MODE == \"quantiles\":\n",
        "    regimes_overall = df.groupby(\"task\", as_index=True).apply(regimes_quantiles).sort_index()\n",
        "    regimes_fold = df.groupby([\"task\", \"fold\"], as_index=True).apply(regimes_quantiles).sort_index()\n",
        "else:\n",
        "    regimes_overall = df.groupby(\"task\", as_index=True).apply(lambda g: regimes_thresholds(g, THR)).sort_index()\n",
        "    regimes_fold = df.groupby([\"task\", \"fold\"], as_index=True).apply(lambda g: regimes_thresholds(g, THR)).sort_index()\n",
        "\n",
        "reg_overall_path = os.path.join(OUT_DIR, f\"regimes_overall_{REGIME_MODE}.csv\")\n",
        "reg_fold_path = os.path.join(OUT_DIR, f\"regimes_fold_{REGIME_MODE}.csv\")\n",
        "\n",
        "regimes_overall.to_csv(reg_overall_path, index=True)\n",
        "regimes_fold.to_csv(reg_fold_path, index=True)\n",
        "\n",
        "regimes_overall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 10 — Correlations (simple + report-table style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(   task  pearson_rawLev_vs_len  pearson_normLev_vs_len\n",
              " 0  sent               0.139599               -0.032001\n",
              " 1  word               0.202630                0.005674,\n",
              "    task  rho_d_all  rho_dtilde_all  rho_d_dpos  rho_dtilde_dpos  n_all  n_dpos\n",
              " 0  sent   0.139599       -0.032001    0.663745        -0.079558  20639    4492\n",
              " 1  word   0.202630        0.005674    0.623748        -0.104828  24163    4976)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simple per-task Pearson correlations (raw d vs len, and lev_norm vs len)\n",
        "corr_rows = []\n",
        "for task, g in df.groupby(\"task\"):\n",
        "    try:\n",
        "        r_raw = np.corrcoef(g[\"levenshtein_distance\"], g[\"label_chars\"])[0, 1]\n",
        "    except Exception:\n",
        "        r_raw = np.nan\n",
        "    try:\n",
        "        r_norm = np.corrcoef(g[\"lev_norm\"], g[\"label_chars\"])[0, 1]\n",
        "    except Exception:\n",
        "        r_norm = np.nan\n",
        "    corr_rows.append({\"task\": task, \"pearson_rawLev_vs_len\": r_raw, \"pearson_normLev_vs_len\": r_norm})\n",
        "\n",
        "length_corr_simple = pd.DataFrame(corr_rows).sort_values(\"task\")\n",
        "length_corr_simple_path = os.path.join(OUT_DIR, \"length_correlation_raw_vs_norm.csv\")\n",
        "length_corr_simple.to_csv(length_corr_simple_path, index=False)\n",
        "\n",
        "# Report-table style (All vs d>0 only)\n",
        "def corr_len_block(g: pd.DataFrame) -> dict:\n",
        "    rho_d_all = g[\"levenshtein_distance\"].corr(g[\"label_chars\"], method=\"pearson\")\n",
        "    rho_e_all = g[\"lev_norm\"].corr(g[\"label_chars\"], method=\"pearson\")\n",
        "\n",
        "    g_bad = g[g[\"levenshtein_distance\"] > 0]\n",
        "    if len(g_bad) >= 2:\n",
        "        rho_d_dpos = g_bad[\"levenshtein_distance\"].corr(g_bad[\"label_chars\"], method=\"pearson\")\n",
        "        rho_e_dpos = g_bad[\"lev_norm\"].corr(g_bad[\"label_chars\"], method=\"pearson\")\n",
        "    else:\n",
        "        rho_d_dpos = np.nan\n",
        "        rho_e_dpos = np.nan\n",
        "\n",
        "    return {\n",
        "        \"rho_d_all\": float(rho_d_all) if pd.notna(rho_d_all) else np.nan,\n",
        "        \"rho_dtilde_all\": float(rho_e_all) if pd.notna(rho_e_all) else np.nan,\n",
        "        \"rho_d_dpos\": float(rho_d_dpos) if pd.notna(rho_d_dpos) else np.nan,\n",
        "        \"rho_dtilde_dpos\": float(rho_e_dpos) if pd.notna(rho_e_dpos) else np.nan,\n",
        "        \"n_all\": int(len(g)),\n",
        "        \"n_dpos\": int((g[\"levenshtein_distance\"] > 0).sum()),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "for task, g in df.groupby(\"task\", sort=True):\n",
        "    r = corr_len_block(g)\n",
        "    r[\"task\"] = task\n",
        "    rows.append(r)\n",
        "\n",
        "corr_df = pd.DataFrame(rows)[\n",
        "    [\"task\", \"rho_d_all\", \"rho_dtilde_all\", \"rho_d_dpos\", \"rho_dtilde_dpos\", \"n_all\", \"n_dpos\"]\n",
        "].sort_values(\"task\")\n",
        "\n",
        "corr_df_path = os.path.join(OUT_DIR, \"corr_len_vs_error_by_task.csv\")\n",
        "corr_df.to_csv(corr_df_path, index=False)\n",
        "\n",
        "length_corr_simple, corr_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 11 — Generate plots per task (PNG files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plots saved in: ./quant_analysis_outputs_new\n"
          ]
        }
      ],
      "source": [
        "for task, g in df.groupby(\"task\"):\n",
        "    plot_exact_rate_by_fold(task, g, OUT_DIR)\n",
        "    plot_histograms_and_cdfs(task, g, OUT_DIR, x_clip=X_CLIP)\n",
        "\n",
        "print(\"Plots saved in:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 12 — Build UID, suspicious flags, and optional recompute check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suspicious rows: 0\n",
            "D mismatches among suspicious: 0\n"
          ]
        }
      ],
      "source": [
        "df[\"uid\"] = df[\"task\"] + \"|\" + df[\"fold\"].astype(str) + \"|\" + df[\"sample_index\"].astype(str)\n",
        "\n",
        "# Debug visibility\n",
        "df[\"pred_repr\"] = df[\"pred_nfc\"].map(show_repr)\n",
        "df[\"label_repr\"] = df[\"label_nfc\"].map(show_repr)\n",
        "df[\"pred_len\"] = df[\"pred_nfc\"].str.len()\n",
        "df[\"label_len\"] = df[\"label_nfc\"].str.len()\n",
        "\n",
        "# Suspicious flags\n",
        "df[\"looks_equal_but_dpos\"] = (df[\"pred_nfc\"] == df[\"label_nfc\"]) & (df[\"levenshtein_distance\"] > 0)\n",
        "df[\"looks_unequal_but_dzero\"] = (df[\"pred_nfc\"] != df[\"label_nfc\"]) & (df[\"levenshtein_distance\"] == 0)\n",
        "df[\"is_suspicious\"] = df[\"looks_equal_but_dpos\"] | df[\"looks_unequal_but_dzero\"]\n",
        "\n",
        "# Optional recompute on suspicious rows only\n",
        "df[\"lev_recomputed_nfc\"] = np.nan\n",
        "df[\"d_mismatch_flag\"] = False\n",
        "\n",
        "if VERIFY_D_WITH_RECOMPUTE:\n",
        "    sus_idx = df.index[df[\"is_suspicious\"]].tolist()\n",
        "    if sus_idx:\n",
        "        recomputed = []\n",
        "        for i in sus_idx:\n",
        "            recomputed.append(levenshtein(df.at[i, \"pred_nfc\"], df.at[i, \"label_nfc\"]))\n",
        "        df.loc[sus_idx, \"lev_recomputed_nfc\"] = recomputed\n",
        "        df.loc[sus_idx, \"d_mismatch_flag\"] = (\n",
        "            df.loc[sus_idx, \"lev_recomputed_nfc\"].astype(int) != df.loc[sus_idx, \"levenshtein_distance\"].astype(int)\n",
        "        )\n",
        "\n",
        "print(\"Suspicious rows:\", int(df[\"is_suspicious\"].sum()))\n",
        "print(\"D mismatches among suspicious:\", int(df[\"d_mismatch_flag\"].sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 13 — Create report-friendly truncated columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task</th>\n",
              "      <th>fold</th>\n",
              "      <th>sample_index</th>\n",
              "      <th>levenshtein_distance</th>\n",
              "      <th>lev_norm</th>\n",
              "      <th>prediction_short</th>\n",
              "      <th>label_short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Du bist dran.</td>\n",
              "      <td>Du bist dran.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Lass/Lasst uns hingehen!; Los, gehe...</td>\n",
              "      <td>Lass/Lasst uns hingehen!; Los, gehe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Ich hole sie/ihn.</td>\n",
              "      <td>Ich hole sie/ihn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Hospital; Krankenhaus</td>\n",
              "      <td>Hospital; Krankenhaus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sent</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>schon einmal; vorher; zuvor</td>\n",
              "      <td>schon einmal; vorher; zuvor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   task  fold  sample_index  levenshtein_distance  lev_norm  \\\n",
              "0  sent     0             0                     0       0.0   \n",
              "1  sent     0             1                     0       0.0   \n",
              "2  sent     0             2                     0       0.0   \n",
              "3  sent     0             3                     0       0.0   \n",
              "4  sent     0             4                     0       0.0   \n",
              "\n",
              "                         prediction_short  \\\n",
              "0                           Du bist dran.   \n",
              "1  Lass/Lasst uns hingehen!; Los, gehe...   \n",
              "2                       Ich hole sie/ihn.   \n",
              "3                   Hospital; Krankenhaus   \n",
              "4             schon einmal; vorher; zuvor   \n",
              "\n",
              "                              label_short  \n",
              "0                           Du bist dran.  \n",
              "1  Lass/Lasst uns hingehen!; Los, gehe...  \n",
              "2                       Ich hole sie/ihn.  \n",
              "3                   Hospital; Krankenhaus  \n",
              "4             schon einmal; vorher; zuvor  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"prediction_full\"] = df[\"prediction\"]\n",
        "df[\"label_full\"] = df[\"label\"]\n",
        "df[\"prediction_short\"] = df[\"prediction\"].map(lambda s: trunc(s, MAX_SHOW))\n",
        "df[\"label_short\"] = df[\"label\"].map(lambda s: trunc(s, MAX_SHOW))\n",
        "\n",
        "df[[\"task\",\"fold\",\"sample_index\",\"levenshtein_distance\",\"lev_norm\",\"prediction_short\",\"label_short\"]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 14 — Quantile examples + tail examples + quantile targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "examples_all: (30, 30)\n",
            "qvals_all: (6, 6)\n",
            "tail_all: (16, 27)\n"
          ]
        }
      ],
      "source": [
        "all_examples = []\n",
        "all_qvals_rows = []\n",
        "all_tail = []\n",
        "\n",
        "for task, g in df.groupby(\"task\", sort=True):\n",
        "    ex_df, qvals = quantile_examples_for_task(g, K_PER_QUANTILE, QUANTILES)\n",
        "    tail_df = tail_topk_for_task(g, TOPK_TAIL)\n",
        "\n",
        "    n_nonzero = int((g[\"lev_norm\"] > 0).sum())\n",
        "    for q, v in qvals.items():\n",
        "        all_qvals_rows.append({\n",
        "            \"task\": task,\n",
        "            \"quantile\": q,\n",
        "            \"quantile_value\": v,\n",
        "            \"n_nonzero\": n_nonzero,\n",
        "            \"n_total\": int(len(g)),\n",
        "            \"exact_match_%\": float(100.0 * (g[\"levenshtein_distance\"] == 0).mean()),\n",
        "        })\n",
        "\n",
        "    if not ex_df.empty:\n",
        "        all_examples.append(ex_df)\n",
        "    if not tail_df.empty:\n",
        "        all_tail.append(tail_df)\n",
        "\n",
        "examples_all = pd.concat(all_examples, ignore_index=True) if all_examples else pd.DataFrame()\n",
        "qvals_all = pd.DataFrame(all_qvals_rows) if all_qvals_rows else pd.DataFrame(\n",
        "    columns=[\"task\", \"quantile\", \"quantile_value\", \"n_nonzero\", \"n_total\", \"exact_match_%\"]\n",
        ")\n",
        "tail_all = pd.concat(all_tail, ignore_index=True) if all_tail else pd.DataFrame()\n",
        "\n",
        "print(\"examples_all:\", examples_all.shape)\n",
        "print(\"qvals_all:\", qvals_all.shape)\n",
        "print(\"tail_all:\", tail_all.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 15 — Select final columns and write CSV outputs (examples/tail/suspicious)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote: ./quant_analysis_outputs_new/examples_by_quantile_all_tasks.csv\n",
            "Wrote: ./quant_analysis_outputs_new/quantile_targets_nonzero_by_task.csv\n",
            "Wrote: ./quant_analysis_outputs_new/examples_tail_topk_by_task.csv\n",
            "Wrote: ./quant_analysis_outputs_new/suspicious_rows_debug.csv\n",
            "Suspicious rows: 0\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# CLEAN COLUMN SETS\n",
        "# - Remove all flag/debug columns from examples + tail except is_suspicious\n",
        "# - Keep FULL text (prediction_full/label_full), drop short text\n",
        "# - Keep suspicious_rows_debug.csv AS IS\n",
        "# =========================\n",
        "\n",
        "examples_cols = [\n",
        "    \"task\",\n",
        "    \"target_quantile\", \"target_value\",\n",
        "    \"fold\", \"sample_index\",\n",
        "    \"levenshtein_distance\", \"label_chars\", \"lev_norm\",\n",
        "    \"prediction_full\", \"label_full\",\n",
        "    \"is_suspicious\",\n",
        "    \"json_path\"\n",
        "]\n",
        "\n",
        "tail_cols = [\n",
        "    \"task\", \"fold\", \"sample_index\",\n",
        "    \"levenshtein_distance\", \"label_chars\", \"lev_norm\",\n",
        "    \"prediction_full\", \"label_full\",\n",
        "    \"is_suspicious\",\n",
        "    \"json_path\"\n",
        "]\n",
        "\n",
        "# NOTE: Keep suspicious debug CSV exactly as before (do not change sus_cols)\n",
        "sus_cols = [\n",
        "    \"task\",\"fold\",\"sample_index\",\n",
        "    \"levenshtein_distance\",\"lev_norm\",\n",
        "    \"prediction_short\",\"label_short\",\n",
        "    \"pred_repr\",\"label_repr\",\n",
        "    \"looks_equal_but_dpos\",\"looks_unequal_but_dzero\",\n",
        "    \"lev_recomputed_nfc\",\"d_mismatch_flag\",\n",
        "    \"json_path\"\n",
        "]\n",
        "\n",
        "# Ensure frames exist with correct schema\n",
        "if examples_all.empty:\n",
        "    examples_all = pd.DataFrame(columns=examples_cols)\n",
        "else:\n",
        "    # Ensure required columns exist\n",
        "    for c in examples_cols:\n",
        "        if c not in examples_all.columns:\n",
        "            examples_all[c] = np.nan\n",
        "\n",
        "    examples_all = examples_all[examples_cols].sort_values(\n",
        "        [\"task\", \"target_quantile\", \"lev_norm\"],\n",
        "        ascending=[True, True, True]\n",
        "    )\n",
        "\n",
        "if tail_all.empty:\n",
        "    tail_all = pd.DataFrame(columns=tail_cols)\n",
        "else:\n",
        "    for c in tail_cols:\n",
        "        if c not in tail_all.columns:\n",
        "            tail_all[c] = np.nan\n",
        "\n",
        "    tail_all = tail_all[tail_cols].sort_values(\n",
        "        [\"task\", \"lev_norm\"],\n",
        "        ascending=[True, False]\n",
        "    )\n",
        "\n",
        "# Keep suspicious_rows_debug.csv as it is\n",
        "sus = df[df[\"is_suspicious\"]].copy()\n",
        "if sus.empty:\n",
        "    sus = pd.DataFrame(columns=sus_cols)\n",
        "else:\n",
        "    for c in sus_cols:\n",
        "        if c not in sus.columns:\n",
        "            sus[c] = np.nan\n",
        "\n",
        "    sus = sus[sus_cols].sort_values(\n",
        "        [\"d_mismatch_flag\", \"task\", \"fold\", \"sample_index\"],\n",
        "        ascending=[False, True, True, True]\n",
        "    )\n",
        "\n",
        "# Write outputs\n",
        "out_examples = os.path.join(OUT_DIR, \"examples_by_quantile_all_tasks.csv\")\n",
        "out_qvals = os.path.join(OUT_DIR, \"quantile_targets_nonzero_by_task.csv\")\n",
        "out_tail = os.path.join(OUT_DIR, \"examples_tail_topk_by_task.csv\")\n",
        "out_susp = os.path.join(OUT_DIR, \"suspicious_rows_debug.csv\")\n",
        "\n",
        "examples_all.to_csv(out_examples, index=False)\n",
        "qvals_all.to_csv(out_qvals, index=False)\n",
        "tail_all.to_csv(out_tail, index=False)\n",
        "sus.to_csv(out_susp, index=False)\n",
        "\n",
        "print(\"Wrote:\", out_examples)\n",
        "print(\"Wrote:\", out_qvals)\n",
        "print(\"Wrote:\", out_tail)\n",
        "print(\"Wrote:\", out_susp)\n",
        "print(\"Suspicious rows:\", len(sus))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 16 — Quick “what was created?” check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files in OUT_DIR: 20\n",
            "CSV: corr_len_vs_error_by_task.csv\n",
            "CSV: examples_by_quantile_all_tasks.csv\n",
            "CSV: examples_tail_topk_by_task.csv\n",
            "CSV: foldwise_summary_norm.csv\n",
            "CSV: length_correlation_raw_vs_norm.csv\n",
            "CSV: overall_summary_norm.csv\n",
            "CSV: quantile_targets_nonzero_by_task.csv\n",
            "CSV: regimes_fold_quantiles.csv\n",
            "CSV: regimes_overall_quantiles.csv\n",
            "CSV: suspicious_rows_debug.csv\n",
            "\n",
            "PNG count: 10\n",
            "First 10 PNGs: ['sent_exact_rate_by_fold.png', 'sent_levnorm_cdf_unclipped.png', 'sent_levnorm_cdf_xmax1.5.png', 'sent_levnorm_hist_clip1.5.png', 'sent_levnorm_hist_unclipped.png', 'word_exact_rate_by_fold.png', 'word_levnorm_cdf_unclipped.png', 'word_levnorm_cdf_xmax1.5.png', 'word_levnorm_hist_clip1.5.png', 'word_levnorm_hist_unclipped.png']\n"
          ]
        }
      ],
      "source": [
        "# List the main CSV outputs and a few plot files\n",
        "files = sorted(os.listdir(OUT_DIR))\n",
        "print(\"Total files in OUT_DIR:\", len(files))\n",
        "\n",
        "# Show key CSVs first\n",
        "for f in files:\n",
        "    if f.endswith(\".csv\"):\n",
        "        print(\"CSV:\", f)\n",
        "\n",
        "# Show a few PNGs\n",
        "pngs = [f for f in files if f.endswith(\".png\")]\n",
        "print(\"\\nPNG count:\", len(pngs))\n",
        "print(\"First 10 PNGs:\", pngs[:10])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
