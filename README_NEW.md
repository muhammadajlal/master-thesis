# IMU Handwriting Recognition (IMU-HWR)

**Writer-Independent Online Handwriting Recognition from Inertial Measurement Unit (IMU) Time Series**

This repository implements multiple neural network architectures for handwriting recognition from IMU sensor data, supporting:
- **CTC decoding** (CNN encoder + BiLSTM/Transformer)
- **Autoregressive (AR) decoding** (CNN encoder + Transformer decoder)
- **Multimodal LM decoding** (CNN encoder + HuggingFace T5/ByT5)

---

## ğŸ“ Project Structure

```
work/REWI_work/
â”œâ”€â”€ main.py                 # Main training/evaluation entry point
â”œâ”€â”€ evaluate.py             # Cross-validation aggregation
â”œâ”€â”€ pretrain_decoder.py     # Text-only decoder pretraining
â”‚
â”œâ”€â”€ configs/                # YAML configuration files
â”‚   â”œâ”€â”€ train.yaml          # Base training config
â”‚   â”œâ”€â”€ test.yaml           # Evaluation config
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ rewi/                   # Core library
â”‚   â”œâ”€â”€ model/              # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py     # BaseModel, build_encoder
â”‚   â”‚   â”œâ”€â”€ conv.py         # CNN encoders (ConvNeXt, etc.)
â”‚   â”‚   â”œâ”€â”€ transformer.py  # Transformer decoder
â”‚   â”‚   â”œâ”€â”€ ARDecoder.py    # Autoregressive decoder
â”‚   â”‚   â””â”€â”€ multimodal_lm_model.py  # HuggingFace LM integration
â”‚   â”‚
â”‚   â”œâ”€â”€ dataset/            # Data loading
â”‚   â”‚   â”œâ”€â”€ __init__.py     # HRDataset
â”‚   â”‚   â”œâ”€â”€ utils.py        # Collate functions
â”‚   â”‚   â””â”€â”€ lm_collate.py   # LM-specific collation
â”‚   â”‚
â”‚   â”œâ”€â”€ training/           # Training loops and utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loops.py        # train_one_epoch, test functions
â”‚   â”‚   â””â”€â”€ utils.py        # Freeze/unfreeze, optimizer helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/           # Qualitative & quantitative analysis
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ attention.py    # Cross-attention visualization
â”‚   â”‚   â”œâ”€â”€ gradcam.py      # Grad-CAM 1D for encoder
â”‚   â”‚   â”œâ”€â”€ selection.py    # Sample selection by quantiles
â”‚   â”‚   â””â”€â”€ metrics.py      # Levenshtein distance, CER
â”‚   â”‚
â”‚   â”œâ”€â”€ tokenizer/          # Text tokenization
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py         # BaseTokenizer interface
â”‚   â”‚   â”œâ”€â”€ bpe.py          # SentencePiece BPE tokenizer
â”‚   â”‚   â”œâ”€â”€ char.py         # Character-level tokenizer
â”‚   â”‚   â””â”€â”€ utils.py        # Text normalization
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluate.py         # Evaluation metrics (CER, WER)
â”‚   â”œâ”€â”€ visualize.py        # Result visualization
â”‚   â”œâ”€â”€ manager.py          # RunManager for logging/checkpointing
â”‚   â”œâ”€â”€ loss.py             # CTC loss wrapper
â”‚   â””â”€â”€ ctc_decoder.py      # CTC best-path decoder
â”‚
â”œâ”€â”€ analysis/               # Analysis scripts and outputs
â”‚   â”œâ”€â”€ scripts/            # Quantitative analysis scripts
â”‚   â””â”€â”€ notebooks/          # Jupyter notebooks
â”‚
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â””â”€â”€ tools/              # Plotting, preprocessing tools
â”‚
â””â”€â”€ slurm/                  # Cluster job scripts
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
conda create -n rewi python=3.10
conda activate rewi
pip install -r requirements.txt
```

### 2. Data Layout

Datasets follow MSCOCO-like structure under `data/`:
```
data/
â””â”€â”€ wi_word_hw6_meta/
    â”œâ”€â”€ train.json          # Training annotations (by fold)
    â”œâ”€â”€ val.json            # Validation annotations (by fold)
    â””â”€â”€ data/               # Sensor CSV files
```

### 3. Training

```bash
cd work/REWI_work
python main.py -c configs/train.yaml
```

### 4. Evaluation

```bash
# Single fold evaluation
python main.py -c configs/test.yaml

# Aggregate cross-validation results
python evaluate.py -c configs/train.yaml
```

---

## âš™ï¸ Configuration

Key YAML parameters:

| Parameter | Description |
|-----------|-------------|
| `idx_fold` | Fold index (0-4, or -1 for all) |
| `dir_dataset` | Path to dataset folder |
| `dir_work` | Output directory for checkpoints/logs |
| `arch_en` | Encoder architecture (e.g., `blconv_b`) |
| `arch_de` | Decoder architecture (e.g., `transformer_s`, `ar_transformer_s`, `t5-small`) |
| `lr` | Learning rate |
| `epoch` | Number of training epochs |
| `size_batch` | Batch size |

### Training Modes

**CTC Mode** (`arch_de: transformer_s`):
```yaml
arch_en: blconv_b
arch_de: transformer_s
```

**AR Mode** (`arch_de: ar_transformer_*`):
```yaml
arch_en: blconv_b
arch_de: ar_transformer_s
use_gated_attention: true
gating_type: elementwise
```

**LM Mode** (`arch_de: t5-small` or `byt5_small`):
```yaml
arch_en: blconv_b
arch_de: t5-small
lm_name: google/t5-v1_1-small
lm_train_lm: false
lm_unfreeze_epoch: 60
lr_enc: 1e-4
lr_proj: 1e-4
lr_lm: 1e-5
```

---

## ğŸ“Š Analysis Tools

### Qualitative Analysis

Enable in test config:
```yaml
test: true
qualitative: true
qual_csv: analysis/quant_all_val_predictions.csv
qual_outdir: qualitative_outputs
qual_use_gradcam: true
```

This generates:
- Cross-attention heatmaps
- Grad-CAM 1D visualizations
- Sample selection by error quantiles (correct, near-miss, catastrophic)

### Quantitative Analysis

```bash
# Generate unified predictions CSV
python analysis/scripts/quant_analysis.py

# Analyze by error quantiles
python analysis/scripts/quant_analysis_2.py
```

---

## ğŸ”§ Extending the Codebase

### Adding a New Encoder

1. Create `rewi/model/my_encoder.py`
2. Register in `rewi/model/__init__.py`:
   ```python
   ENCODERS['my_encoder'] = MyEncoder
   ```

### Adding a New Tokenizer

1. Inherit from `rewi.tokenizer.BaseTokenizer`
2. Implement `encode()`, `decode()`, and special token properties
3. Register in `rewi/tokenizer/__init__.py`

### Adding Analysis Methods

1. Add to appropriate module in `rewi/analysis/`
2. Export in `rewi/analysis/__init__.py`
3. Integrate in `rewi/training/loops.py` if needed during evaluation

---

## ğŸ“š Module Overview

### `rewi.model`
Neural network architectures including CNN encoders (ConvNeXt-style blocks), Transformer decoders, autoregressive decoders with gated attention, and multimodal LM integration.

### `rewi.dataset`
Data loading utilities for IMU time series with support for augmentation, sequence padding, and CTC length constraints.

### `rewi.training`
Training and evaluation loops for CTC, AR, and LM modes with utilities for parameter freezing, optimizer debugging, and checkpoint management.

### `rewi.analysis`
Tools for model interpretability including attention visualization, Grad-CAM for temporal saliency, and error analysis by quantiles.

### `rewi.tokenizer`
Text tokenization with consistent API for BPE (SentencePiece) and character-level encoding. Supports vocabulary building and serialization.

---

## ğŸ“ Citation

If you use this code, please cite:
```bibtex
@article{rewi2025,
  title={Robust and Efficient Writer-Independent IMU-Based Handwriting Recognition},
  author={...},
  journal={arXiv preprint arXiv:2502.20954},
  year={2025}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE.txt](LICENSE.txt) for details.
